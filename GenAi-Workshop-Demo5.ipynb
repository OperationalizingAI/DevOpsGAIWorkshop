{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cKGCZuX3bXo88lJqqUy9bk70wIeW_O02","timestamp":1712772263178},{"file_id":"1bv0fjWMeMtF3f8xI0-__5mMkWEPxI605","timestamp":1705687665918}],"authorship_tag":"ABX9TyNcsT9+D4LHaVe/n5wgdKRZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%pip install --quiet langchain pypdf pymongo langchain-openai tiktoken datasets google-cloud-secret-manager"],"metadata":{"id":"KeIAv4AWCwop","executionInfo":{"status":"ok","timestamp":1712774680138,"user_tz":240,"elapsed":14780,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%pip install --upgrade google-auth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10yHcBbjXrZb","executionInfo":{"status":"ok","timestamp":1712774701763,"user_tz":240,"elapsed":8163,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"fbded779-5f46-45f5-de74-151735869262"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.29.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.0)\n"]}]},{"cell_type":"code","source":["!pip install arize-phoenix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_tvqlcdBnOR","executionInfo":{"status":"ok","timestamp":1712774721123,"user_tz":240,"elapsed":16003,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"3610d069-d026-427a-f4b3-97fb63eabaa6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: arize-phoenix in /usr/local/lib/python3.10/dist-packages (3.19.4)\n","Requirement already satisfied: hdbscan>=0.8.33 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.8.33)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (3.1.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.25.2)\n","Requirement already satisfied: openinference-instrumentation in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.1.1)\n","Requirement already satisfied: openinference-instrumentation-langchain>=0.1.12 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.1.14)\n","Requirement already satisfied: openinference-instrumentation-llama-index>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.2.1)\n","Requirement already satisfied: openinference-instrumentation-openai>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.1.4)\n","Requirement already satisfied: openinference-semantic-conventions>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.1.5)\n","Requirement already satisfied: opentelemetry-exporter-otlp in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.24.0)\n","Requirement already satisfied: opentelemetry-proto in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.24.0)\n","Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.24.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (2.0.3)\n","Requirement already satisfied: protobuf<6.0,>=3.20 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (3.20.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (5.9.5)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (14.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (2.31.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.11.4)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (2.4.0)\n","Requirement already satisfied: starlette in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.37.2)\n","Requirement already satisfied: strawberry-graphql==0.208.2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.208.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (4.66.2)\n","Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (4.10.0)\n","Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.5.6)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.29.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.14.1)\n","Requirement already satisfied: graphql-core<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.208.2->arize-phoenix) (3.2.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.208.2->arize-phoenix) (2.8.2)\n","Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.33->arize-phoenix) (0.29.37)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.33->arize-phoenix) (1.3.2)\n","Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-langchain>=0.1.12->arize-phoenix) (1.24.0)\n","Requirement already satisfied: opentelemetry-instrumentation in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-langchain>=0.1.12->arize-phoenix) (0.45b0)\n","Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-langchain>=0.1.12->arize-phoenix) (0.45b0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->arize-phoenix) (3.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->arize-phoenix) (2.1.5)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.24.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.24.0)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->arize-phoenix) (1.2.14)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->arize-phoenix) (1.63.0)\n","Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->arize-phoenix) (1.62.1)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->arize-phoenix) (1.24.0)\n","Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->openinference-instrumentation-langchain>=0.1.12->arize-phoenix) (7.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->arize-phoenix) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->arize-phoenix) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->arize-phoenix) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->arize-phoenix) (2024.2.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->arize-phoenix) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->arize-phoenix) (2024.1)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->arize-phoenix) (3.7.1)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn->arize-phoenix) (0.58.1)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn->arize-phoenix) (0.5.12)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->arize-phoenix) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->arize-phoenix) (0.14.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix) (1.2.0)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn->arize-phoenix) (0.41.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.208.2->arize-phoenix) (1.16.0)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-langchain>=0.1.12->arize-phoenix) (67.7.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api->openinference-instrumentation-langchain>=0.1.12->arize-phoenix) (3.18.1)\n"]}]},{"cell_type":"code","source":["from urllib.request import urlopen\n","\n","import nest_asyncio\n","import numpy as np\n","import pandas as pd\n","import phoenix as px\n","from langchain.chains import RetrievalQA\n","from langchain.chat_models import ChatOpenAI\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.retrievers import KNNRetriever\n","\n","import datasets\n","from datasets import load_dataset\n","\n","from phoenix.experimental.evals import (\n","    HallucinationEvaluator,\n","    OpenAIModel,\n","    QAEvaluator,\n","    RelevanceEvaluator,\n","    run_evals,\n",")\n","from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n","from phoenix.trace import DocumentEvaluations, SpanEvaluations\n","from phoenix.trace.langchain import LangChainInstrumentor\n","from tqdm import tqdm\n","\n","nest_asyncio.apply()  # needed for concurrent evals in notebook environments"],"metadata":{"id":"cq-TahYFBhZD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712774779848,"user_tz":240,"elapsed":30461,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"001864e9-6add-4840-e6d4-93d25a29e6e0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:phoenix.experimental.evals:Evals are moving out of experimental. Install the evals extra with `pip install arize-phoenix[evals]` and import `phoenix.evals`. For more info, see the [migration guide](https://github.com/Arize-ai/phoenix/blob/main/MIGRATION.md).\n"]}]},{"cell_type":"code","source":["import os\n","\n","%pip show langchain\n","\n","from platform import python_version\n","print(python_version())"],"metadata":{"id":"qianBJwuzkeL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712774826584,"user_tz":240,"elapsed":6523,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"96ece62a-8611-4670-fdc4-32769765a456"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: langchain\n","Version: 0.1.15\n","Summary: Building applications with LLMs through composability\n","Home-page: https://github.com/langchain-ai/langchain\n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n","Required-by: \n","3.10.12\n"]}]},{"cell_type":"code","source":["from google.cloud import secretmanager\n","from google.colab import auth\n","from google.colab import drive"],"metadata":{"id":"bjhBAbD8ayLq","executionInfo":{"status":"ok","timestamp":1712774832190,"user_tz":240,"elapsed":1253,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def load_secrets(secrets_name, project_id):\n","  # Build a client\n","  auth.authenticate_user()\n","  client = secretmanager.SecretManagerServiceClient()\n","  secret_name = secrets_name\n","  # Create path to latest secret\n","  resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"\n","  # Get your secret :\n","  response = client.access_secret_version(request={\"name\": resource_name})\n","  secret_string = response.payload.data.decode('UTF-8')\n","  return secret_string"],"metadata":{"id":"t-gm53pja_56","executionInfo":{"status":"ok","timestamp":1712774837231,"user_tz":240,"elapsed":187,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["project_id = 'botchagalupep1'\n","openai_api_key = load_secrets(\"openai_api_key\",project_id)\n","os.environ['OPENAI_API_KEY'] = openai_api_key\n","#MONGODB_ATLAS_CLUSTER_URI = load_secrets(\"mdb_uri\",project_id)\n","MONGODB_ATLAS_CLUSTER_URI = load_secrets(\"MDB_CLUSTER0_URI\",project_id)\n","langsmith_api_key = load_secrets(\"langsmith_api_key\",project_id)\n","#print(langsmith_api_key )"],"metadata":{"id":"Jp1WKMLTbFXO","executionInfo":{"status":"ok","timestamp":1712774857813,"user_tz":240,"elapsed":17641,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Now we will setup the environment variables for the MongoDB Atlas cluster\n"],"metadata":{"id":"m1GSMqJL4Z7G"}},{"cell_type":"code","source":["from pymongo import MongoClient\n","\n","# initialize MongoDB python client\n","client = MongoClient(MONGODB_ATLAS_CLUSTER_URI)\n","\n","DB_NAME = \"Cluster0\"\n","COLLECTION_NAME = \"Langchain_demo\"\n","ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n","\n","MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]"],"metadata":{"id":"6ydwxNBP3alb","executionInfo":{"status":"ok","timestamp":1712774877239,"user_tz":240,"elapsed":394,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":[" In the below example, embedding is the name of the field that contains the embedding vector. Please refer to the documentation to get more details on how to define an Atlas Vector Search index. You can name the index {ATLAS_VECTOR_SEARCH_INDEX_NAME} and create the index on the namespace {DB_NAME}.{COLLECTION_NAME}. Finally, write the following definition in the JSON editor on MongoDB Atlas:\n","\n","~~~\n","{\n"," \"fields\": [{\n","   \"type\": \"vector\",\n","   \"path\": \"embedding\",\n","   \"numDimensions\": 1536,\n","   \"similarity\": \"cosine\"\n"," }]\n","}\n","\n"],"metadata":{"id":"DpvqoXey5Gi2"}},{"cell_type":"markdown","source":["#Load Data"],"metadata":{"id":"MElNfK8r5WWS"}},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFLoader\n","\n","# Load the PDF\n","loader = PyPDFLoader(\"https://arxiv.org/pdf/2303.08774.pdf\")\n","data = loader.load()"],"metadata":{"id":"cYe3-EGH5VDQ","executionInfo":{"status":"ok","timestamp":1712774885092,"user_tz":240,"elapsed":3504,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n","docs = text_splitter.split_documents(data)"],"metadata":{"id":"dOuGV0ow5clB","executionInfo":{"status":"ok","timestamp":1712774891523,"user_tz":240,"elapsed":156,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(docs[0])"],"metadata":{"id":"4TxFGS535fW_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712774895768,"user_tz":240,"elapsed":365,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"c9ac9b97-5f45-419c-9561-2ce5087bae07"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["page_content='GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction' metadata={'source': 'https://arxiv.org/pdf/2303.08774.pdf', 'page': 0}\n"]}]},{"cell_type":"code","source":["from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n","from langchain_openai import OpenAIEmbeddings\n","\n","# insert the documents in MongoDB Atlas with their embedding\n","vector_search = MongoDBAtlasVectorSearch.from_documents(\n","    documents=docs,\n","    embedding=OpenAIEmbeddings(disallowed_special=()),\n","    collection=MONGODB_COLLECTION,\n","    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",")"],"metadata":{"id":"JUVZbKK_5llj","executionInfo":{"status":"ok","timestamp":1712774911691,"user_tz":240,"elapsed":11935,"user":{"displayName":"John Willis","userId":"15271974867993570949"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Perform a similarity search between the embedding of the query and the embeddings of the documents\n","query = \"What is GPT-4?\"\n","results = vector_search.similarity_search(query)\n","\n","print(results[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haHly202NRJl","executionInfo":{"status":"ok","timestamp":1712774962686,"user_tz":240,"elapsed":514,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"f10905fa-9719-4cb4-dc9b-6c448d1bd2cf"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["GPT-4 Technical Report\n","OpenAI∗\n","Abstract\n","We report the development of GPT-4, a large-scale, multimodal model which can\n","accept image and text inputs and produce text outputs. While less capable than\n","humans in many real-world scenarios, GPT-4 exhibits human-level performance\n","on various professional and academic benchmarks, including passing a simulated\n","bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\n","based model pre-trained to predict the next token in a document. The post-training\n","alignment process results in improved performance on measures of factuality and\n","adherence to desired behavior. A core component of this project was developing\n","infrastructure and optimization methods that behave predictably across a wide\n","range of scales. This allowed us to accurately predict some aspects of GPT-4’s\n","performance based on models trained with no more than 1/1,000th the compute of\n","GPT-4.\n","1 Introduction\n"]}]}]}