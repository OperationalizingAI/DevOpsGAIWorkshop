{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bv0fjWMeMtF3f8xI0-__5mMkWEPxI605","timestamp":1705687706179}],"authorship_tag":"ABX9TyPcN4tQ1u4lkR014u0KR6Vk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%pip install --quiet langchain pypdf pymongo langchain-openai tiktoken"],"metadata":{"id":"KeIAv4AWCwop","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705687956904,"user_tz":360,"elapsed":38670,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"b8cbd7fa-a8f7-40d7-f583-85c17c7a9cd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.9/283.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m922.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","\n","%pip show langchain\n","\n","from platform import python_version\n","print(python_version())"],"metadata":{"id":"qianBJwuzkeL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705688038179,"user_tz":360,"elapsed":6261,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"8463d1b8-9fb9-4b3e-fbee-2a5a7bd7ca66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: langchain\n","Version: 0.1.1\n","Summary: Building applications with LLMs through composability\n","Home-page: https://github.com/langchain-ai/langchain\n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n","Required-by: \n","3.10.12\n"]}]},{"cell_type":"code","source":["import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"\"\n","\n","MONGODB_ATLAS_CLUSTER_URI = \"\""],"metadata":{"id":"bjhBAbD8ayLq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will setup the environment variables for the MongoDB Atlas cluster\n"],"metadata":{"id":"m1GSMqJL4Z7G"}},{"cell_type":"code","source":["from pymongo import MongoClient\n","\n","# initialize MongoDB python client\n","client = MongoClient(MONGODB_ATLAS_CLUSTER_URI)\n","\n","DB_NAME = \"langchain_db\"\n","COLLECTION_NAME = \"test\"\n","ATLAS_VECTOR_SEARCH_INDEX_NAME = \"index_name\"\n","\n","MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]"],"metadata":{"id":"6ydwxNBP3alb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" In the below example, embedding is the name of the field that contains the embedding vector. Please refer to the documentation to get more details on how to define an Atlas Vector Search index. You can name the index {ATLAS_VECTOR_SEARCH_INDEX_NAME} and create the index on the namespace {DB_NAME}.{COLLECTION_NAME}. Finally, write the following definition in the JSON editor on MongoDB Atlas:\n","\n","~~~\n","{\n"," \"fields\": [{\n","   \"type\": \"vector\",\n","   \"path\": \"embedding\",\n","   \"numDimensions\": 1536,\n","   \"similarity\": \"cosine\"\n"," }]\n","}\n","\n"],"metadata":{"id":"DpvqoXey5Gi2"}},{"cell_type":"markdown","source":["#Load Data"],"metadata":{"id":"MElNfK8r5WWS"}},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFLoader\n","\n","# Load the PDF\n","loader = PyPDFLoader(\"https://arxiv.org/pdf/2303.08774.pdf\")\n","data = loader.load()"],"metadata":{"id":"cYe3-EGH5VDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n","docs = text_splitter.split_documents(data)"],"metadata":{"id":"dOuGV0ow5clB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(docs[0])"],"metadata":{"id":"4TxFGS535fW_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705688092655,"user_tz":360,"elapsed":96,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"2de6bfb5-8cd0-463a-c36a-2e0b7998d098"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["page_content='GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction' metadata={'source': 'https://arxiv.org/pdf/2303.08774.pdf', 'page': 0}\n"]}]},{"cell_type":"code","source":["from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n","from langchain_openai import OpenAIEmbeddings\n","\n","# insert the documents in MongoDB Atlas with their embedding\n","vector_search = MongoDBAtlasVectorSearch.from_documents(\n","    documents=docs,\n","    embedding=OpenAIEmbeddings(disallowed_special=()),\n","    collection=MONGODB_COLLECTION,\n","    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",")"],"metadata":{"id":"JUVZbKK_5llj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform a similarity search between the embedding of the query and the embeddings of the documents\n","query = \"What were the compute requirements for training GPT 4\"\n","results = vector_search.similarity_search(query)\n","\n","print(results[0].page_content)"],"metadata":{"id":"zfJefNOo5qxF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705688109611,"user_tz":360,"elapsed":377,"user":{"displayName":"John Willis","userId":"15271974867993570949"}},"outputId":"ffd00fc9-a2ce-48fa-ab76-db4b373b8f3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["performance based on models trained with no more than 1/1,000th the compute of\n","GPT-4.\n","1 Introduction\n","This technical report presents GPT-4, a large multimodal model capable of processing image and\n","text inputs and producing text outputs. Such models are an important area of study as they have the\n","potential to be used in a wide range of applications, such as dialogue systems, text summarization,\n","and machine translation. As such, they have been the subject of substantial interest and progress in\n","recent years [1–34].\n","One of the main goals of developing such models is to improve their ability to understand and generate\n","natural language text, particularly in more complex and nuanced scenarios. To test its capabilities\n","in such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\n","these evaluations it performs quite well and often outscores the vast majority of human test takers.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6z1BxFKU5JZ5"},"execution_count":null,"outputs":[]}]}